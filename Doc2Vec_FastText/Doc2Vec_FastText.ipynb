{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical, Sequence\n",
    "import os\n",
    "import time\n",
    "from gensim.models import FastText, Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = {}\n",
    "with open('dbpedia/dbpedia_csv/classes.txt') as txt_file:\n",
    "    line_num = 1\n",
    "    for line in txt_file:\n",
    "        class_mapping[line_num] = line.strip()\n",
    "        line_num+=1\n",
    "        \n",
    "print(class_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2Vec_FastText_SequenceDataGenerator(Sequence):\n",
    "    def __init__(self, list_IDs, batch_size=32,\n",
    "                 n_classes=14, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = (300,)\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate index of the batch\n",
    "        indexes = self.indexes[index:(index+1)]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        \n",
    "        FT_array = np.empty((self.batch_size, *self.dim))\n",
    "        DV_array = np.empty((self.batch_size, *self.dim))\n",
    "        y = np.empty((self.batch_size), dtype = int)\n",
    "        for item in list_IDs_temp:\n",
    "            Xarray = pkl.load(open('dbpedia/Training/'+item, 'rb'))\n",
    "        for i, row in enumerate(Xarray[1]):\n",
    "            try:\n",
    "                FT_array[i,] = fastText.wv[row[-1]]\n",
    "            except KeyError:\n",
    "                # KeyError is raised when row[-1] is not in the FastText vocabulary\n",
    "                FT_array[i,] = np.zeros((300,))\n",
    "            DV_array[i,] = Doc2Vec_model.infer_vector(row)\n",
    "            y[i] = Xarray[0]-1\n",
    "\n",
    "        return [FT_array,DV_array], to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load FastText and Doc2Vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText = FastText.load_fasttext_format('fasttext.bin')\n",
    "\n",
    "Doc2Vec_model = Doc2Vec.load(\"Doc2Vec.model\")\n",
    "Doc2Vec_model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "\n",
    "partition, labels = pkl.load(open('dbpedia/Training/Metadata', 'rb'))\n",
    "for key in labels.keys():\n",
    "    labels[key] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size':32, 'n_classes': 14, 'shuffle': True}\n",
    "training_generator = Doc2Vec_FastText_SequenceDataGenerator(partition['train'], **params)\n",
    "validation_generator = Doc2Vec_FastText_SequenceDataGenerator(partition['validation'], **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Doc2Vec_FastText model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_input = Input(shape = (300,), name = 'fastText_input')\n",
    "fastText_output = Dense(32, activation = 'relu')(fastText_input)\n",
    "Doc2Vec_input = Input(shape = (300,), name = 'Doc2Vec_input')\n",
    "Doc2Vec_dense = Dense(128, activation = 'relu')(Doc2Vec_input)\n",
    "Doc2Vec_output = Dense(128, activation = 'relu')(Doc2Vec_dense)\n",
    "concatenated = keras.layers.concatenate([fastText_output, Doc2Vec_output])\n",
    "predictions = Dense(14, activation = 'softmax')(concatenated)\n",
    "\n",
    "model = Model(inputs = [fastText_input, Doc2Vec_input], outputs = predictions)\n",
    "model.compile(optimizer ='Nadam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.name = 'Doc2Vec_FastText32'\n",
    "loss_acc_path = model.name+'loss_acc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(generator = training_generator, epochs = 5, validation_data = validation_generator, verbose = 1)\n",
    "loss_acc = {}\n",
    "if os.path.exists(loss_acc_path):\n",
    "    loss_acc = pkl.load(open(loss_acc_path, 'rb'))\n",
    "    loss_acc['acc'] = loss_acc['acc'] + history.history['acc']\n",
    "    loss_acc['loss'] = loss_acc['loss'] + history.history['loss']\n",
    "else:\n",
    "    loss_acc['acc'] = history.history['acc']\n",
    "    loss_acc['loss'] = history.history['loss']\n",
    "pkl.dump(loss_acc,open(loss_acc_path, 'wb'))\n",
    "model.save(model.name+'.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('Doc2Vec_FastText64.hdf5')\n",
    "fastText = FastText.load_fasttext_format('fasttext.bin')\n",
    "Doc2Vec_model = Doc2Vec.load('Doc2Vec.model')\n",
    "Doc2Vec_model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "accuracies = {}\n",
    "for label in os.listdir('dbpedia/Testing'):\n",
    "    accuracies[label] = {}\n",
    "    for sent_len in os.listdir('dbpedia/Testing/{}/'.format(label)):\n",
    "        if int(sent_len) < 100:\n",
    "            X_FT = np.empty((0,300))\n",
    "            X_DV = np.empty((0,300))\n",
    "            labels = []\n",
    "            for file in os.listdir('dbpedia/Testing/{}/{}/'.format(label,sent_len)):\n",
    "                sample = pkl.load(open('dbpedia/Testing/{}/{}/{}'.format(label,sent_len,file), 'rb'))\n",
    "                labels.append(sample[0])\n",
    "                try:\n",
    "                    X_FT = np.vstack((X_FT,fastText.wv[sample[1][-1]]))\n",
    "                except KeyError:\n",
    "                    X_FT = np.vstack((X_FT,np.zeros((300,))))\n",
    "                X_DV = np.vstack((X_DV, Doc2Vec_model.infer_vector(sample[1])))\n",
    "            result = model.predict([X_FT,X_DV])\n",
    "            result = np.argmax(result,axis = 1)+1\n",
    "            accuracy = np.sum(result == labels)/len(labels)\n",
    "            print('{}, sentence length: {}, Accuracy: {}'.format(label,sent_len,np.sum(result == labels)/len(labels)))\n",
    "            accuracies[label][sent_len] = accuracy\n",
    "            print('elapsed time: {}'.format((time.time()-start)/60))\n",
    "    pkl.dump(accuracies, open('dbpedia/Doc2Vec_FastText_Accuracy', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
